
# ğŸ”¥ Open-dLLM: ã‚ªãƒ¼ãƒ—ãƒ³æ‹¡æ•£å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«

ğŸŒ è¨€èª: [English](README.md) | [ä¸­æ–‡](README_cn.md) | [æ—¥æœ¬èª](README_ja.md)

ğŸ‘‰ TL;DR: **Open-dLLM**ã¯ã€æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æœ€ã‚‚ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒªãƒªãƒ¼ã‚¹ã§ã™ â€”
**äº‹å‰å­¦ç¿’ã€è©•ä¾¡ã€æ¨è«–ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**ã®ã™ã¹ã¦ã‚’å«ã¿ã¾ã™ã€‚

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€Open-dLLMã®**ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒãƒªã‚¢ãƒ³ãƒˆ**ã§ã‚ã‚‹**Open-dCoder**ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚


<p align="center">
  <a href="https://github.com/pengzhangzhi/Open-dLLM">
    <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/github/github-original.svg" width="40" alt="GitHub"/>
  </a>
  &nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://oval-shell-31c.notion.site/Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a?pvs=74">
    <img src="https://upload.wikimedia.org/wikipedia/commons/e/e9/Notion-logo.svg" width="40" alt="Notion"/>
  </a>
  &nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://huggingface.co/fredzzp/open-dcoder-0.5B">
    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="40" alt="Hugging Face"/>
  </a>
</p>

<p align="center">
  <b>ğŸ’» ã‚³ãƒ¼ãƒ‰</b> &nbsp; | &nbsp; <b>ğŸ“– ãƒ–ãƒ­ã‚°</b> &nbsp; | &nbsp; <b>ğŸ¤— ãƒ¢ãƒ‡ãƒ«</b>
</p>


## ğŸ¥ ãƒ‡ãƒ¢

<p align="center">
  <img src="https://github.com/pengzhangzhi/dLLM-training/blob/main/assets/quick-sort-demo.gif"
       alt="Quick Sort Demo" width="600"/>
</p>

<p align="center"><i>Open-dCoder (0.5B)ã‚’ä½¿ç”¨ã—ãŸã‚¯ã‚¤ãƒƒã‚¯ã‚½ãƒ¼ãƒˆç”Ÿæˆ</i></p>

<p align="center">
  <a href="https://youtu.be/d8WrmvUhO9g">
    <img src="https://img.shields.io/badge/YouTube-Video-red?logo=youtube" alt="YouTube link"/>
  </a>
  &nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://www.bilibili.com/video/BV1ZveSz3E1J/">
    <img src="https://img.shields.io/badge/Bilibili-è§†é¢‘-blue?logo=bilibili" alt="Bilibili link"/>
  </a>
</p>

---

## âœ¨ ãƒã‚¤ãƒ©ã‚¤ãƒˆ

- ğŸ‹ï¸ **äº‹å‰å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**
- âš¡ **æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ** â€” ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ç”Ÿæˆ
- ğŸ“Š **è©•ä¾¡ã‚¹ã‚¤ãƒ¼ãƒˆ** â€” HumanEvalã€MBPPã€Infillingï¼ˆlm-eval-harness + ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
- ğŸ“¦ **é‡ã¿ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**ã‚’Hugging Faceã§å…¬é–‹
- ğŸ¤ **é€æ˜æ€§ã®ã‚ã‚‹è¨­å®š**ã«ã‚ˆã‚Šå®Œå…¨ãªå†ç¾æ€§ã‚’å®Ÿç¾

---

## ãªãœOpen-dLLMãªã®ã‹ï¼Ÿ

ã»ã¨ã‚“ã©ã®æ‹¡æ•£LLMãƒªãƒã‚¸ãƒˆãƒªï¼ˆä¾‹ï¼šLLaDAã€Dreamï¼‰ã¯**æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨é‡ã¿**ã®ã¿ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¦ãŠã‚Šã€å†ç¾æ€§ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ã€‚
**Open-dLLM**ã¯ã€æ‹¡æ•£LLMã®**å…¨ã‚¹ã‚¿ãƒƒã‚¯**ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ãŸåˆã‚ã¦ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚

ğŸ‘‰ Open-dLLMã‚’ä½¿ãˆã°ã€**ç”Ÿãƒ‡ãƒ¼ã‚¿ â†’ å­¦ç¿’ â†’ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ â†’ è©•ä¾¡ â†’ æ¨è«–**ã®ã™ã¹ã¦ã‚’1ã¤ã®ãƒªãƒã‚¸ãƒˆãƒªã§å®Ÿç¾ã§ãã¾ã™ã€‚

---

## ğŸ” æ‹¡æ•£LLMãƒªãƒªãƒ¼ã‚¹ã®é€æ˜æ€§æ¯”è¼ƒ

| ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ                                                                 | ãƒ‡ãƒ¼ã‚¿ | å­¦ç¿’ã‚³ãƒ¼ãƒ‰ | æ¨è«– | è©•ä¾¡ | é‡ã¿ |
|-------------------------------------------------------------------------|:---:|:-------------:|:---------:|:----------:|:-------:|
| **Open-dLLM / Open-dCoderï¼ˆæœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼‰**                                      | âœ…  | âœ…            | âœ…        | âœ…         | âœ…      |
| [LLaDA](https://github.com/ML-GSAI/LLaDA)                               | âŒ  | âŒ            | âœ…        | âš ï¸ é™å®šçš„ | âœ…      |
| [Dream](https://github.com/HKUNLP/Dream)                                | âŒ  | âŒ            | âœ…        | âš ï¸ é™å®šçš„ | âœ…      |
| [Gemini-Diffusion](https://deepmind.google/models/gemini-diffusion/)    | âŒ  | âŒ            | âŒ        | âŒ         | âŒï¼ˆAPIã®ã¿ï¼‰ |
| [Seed Diffusion](https://seed.bytedance.com/seed_diffusion)             | âŒ  | âŒ            | âŒ        | âŒ         | âŒï¼ˆAPIã®ã¿ï¼‰ |
| [Mercury](https://www.inceptionlabs.ai/introducing-mercury-our-general-chat-model) | âŒ  | âŒ            | âŒ        | âŒ         | âŒï¼ˆAPIã®ã¿ï¼‰ |

âœ… = å®Œå…¨åˆ©ç”¨å¯èƒ½ Â· âŒ = æä¾›ãªã— Â· âš ï¸ = éƒ¨åˆ†çš„/é™å®šçš„

---

## âš™ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ç’°å¢ƒç®¡ç†ã«ã¯`micromamba`ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ï¼ˆ`conda`ã«ã‚‚å¯¾å¿œå¯èƒ½ï¼‰:

```bash
micromamba install -c nvidia/label/cuda-12.3.0 cuda-toolkit -y
pip install ninja

# cu121å¯¾å¿œã®æœ€æ–°torchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch==2.5.0 --index-url https://download.pytorch.org/whl/cu121

pip install "flash-attn==2.7.4.post1" \
  --extra-index-url https://github.com/Dao-AILab/flash-attention/releases/download

pip install --upgrade --no-cache-dir \
  tensordict torchdata triton>=3.1.0 \
  transformers==4.54.1 accelerate datasets peft hf-transfer \
  codetiming hydra-core pandas pyarrow>=15.0.0 pylatexenc \
  wandb ninja liger-kernel==0.5.8 \
  pytest yapf py-spy pyext pre-commit ruff packaging

pip install -e .
```

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
from transformers import AutoTokenizer
from veomni.models.transformers.qwen2.modeling_qwen2 import Qwen2ForCausalLM
from veomni.models.transformers.qwen2.generation_utils import MDMGenerationConfig
import torch

model_id = "fredzzp/open-dcoder-0.5B"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = Qwen2ForCausalLM.from_pretrained(
    model_id, torch_dtype=torch.bfloat16, trust_remote_code=True
).to(device).eval()

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = "Write a quick sort algorithm in python."
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

# ç”Ÿæˆè¨­å®š
gen_cfg = MDMGenerationConfig(max_new_tokens=128, steps=200, temperature=0.7)

with torch.no_grad():
    outputs = model.diffusion_generate(inputs=input_ids, generation_config=gen_cfg)

print(tokenizer.decode(outputs.sequences[0], skip_special_tokens=True))
```

ğŸ‘‰ å®Œå…¨ãªãƒ­ã‚®ãƒ³ã‚°ã€å±¥æ­´è¿½è·¡ã€ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚’è¡Œã†å ´åˆ:

```bash
python sample.py
```

---

## ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®LLMï¼ˆdLLMï¼‰å‘ã‘ã®å®Œå…¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹**è©•ä¾¡ã‚¹ã‚¤ãƒ¼ãƒˆ**ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¦ã„ã¾ã™ã€‚**æ¨™æº–çš„ãªã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¿ã‚¹ã‚¯**ã¨**ã‚³ãƒ¼ãƒ‰è£œå®Œï¼ˆInfillingï¼‰ã‚¿ã‚¹ã‚¯**ã®ä¸¡æ–¹ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚

ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã¯æ¬¡ãŒå«ã¾ã‚Œã¾ã™: **HumanEval / HumanEval+**ã€**MBPP / MBPP+**ã€**HumanEval-Infill**ã€**SantaCoder-FIM**ã€‚

---

#### æ¨™æº–çš„ãªã‚³ãƒ¼ãƒ‰ç”Ÿæˆ

| æ‰‹æ³•                       | HumanEval |          | HumanEval+ |          | MBPP     |          | MBPP+    |          |
| ---------------------------- | --------- | -------- | ---------- | -------- | -------- | -------- | -------- | -------- |
|                              | Pass\@1   | Pass\@10 | Pass\@1    | Pass\@10 | Pass\@1  | Pass\@10 | Pass\@1  | Pass\@10 |
| LLaDA (8B)                   | 35.4      | 50.0     | 30.5       | 43.3     | 38.8     | 53.4        | 52.6     | 69.1        |
| Dream (7B)                   | 56.7      | 59.2     | 50.0       | 53.7     | 55.4     | 56.2        | 71.5     | 72.5        |
| Mask DFM (1.3B)              | 9.1       | 17.6     | 7.9        | 13.4     | 6.2      | 25.0     | â€“        | â€“        |
| Edit Flow (1.3B)             | 12.8      | 24.3     | 10.4       | 20.7     | 10.0     | 36.4     | â€“        | â€“        |
| **Open-dCoder (0.5Bã€æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)** | **20.8**  | **38.4** | **17.6**   | **35.2** | **16.7** | **38.4** | **23.9** | **53.6** |

> *ã‚ãšã‹0.5Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€Open-dCoderã¯ã‚³ãƒ¼ãƒ‰è£œå®Œã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ã¯ã‚‹ã‹ã«å¤§è¦æ¨¡ãªdLLMã¨ç«¶åˆã—ã¾ã™ã€‚*

---

#### ã‚³ãƒ¼ãƒ‰è£œå®Œï¼ˆInfillingï¼‰

| æ‰‹æ³•                                | HumanEval Infill Pass@1 | SantaCoder Exact Match |
| ------------------------------------- | ----------------------: | ---------------------: |
| LLaDA-8B                              |                    48.3 |                  35.1  |
| Dream-7B                              |                    39.4 |                  40.7  |
| DiffuCoder-7B                         |                    54.8 |                  38.8  |
| Dream-Coder-7B                        |                    55.3 |                  40.0  |
| **Open-dCoder (0.5Bã€æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)**          |                    32.5 |                  29.6  |
| **Open-dCoder (0.5Bã€æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)** Oracle Length |               77.4 |                  56.4  |

> *çµæœå–å¾—ã«ã¯ã€[DreamOn](https://hkunlp.github.io/blog/2025/dreamon/)ã®å¹³å‡å›ºå®šé•·è©•ä¾¡è¨­å®šã«å¾“ã„ã¾ã—ãŸã€‚*

---

## ğŸ§ª è©•ä¾¡

è©•ä¾¡ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:

```bash
pip install -e lm-evaluation-harness human-eval-infilling
```

#### ã‚³ãƒ¼ãƒ‰è£œå®Œï¼ˆHumanEvalã€MBPPï¼‰

```bash
cd eval/eval_completion
bash run_eval.sh
```

#### ã‚³ãƒ¼ãƒ‰è£œå®Œï¼ˆInfillingï¼‰

```bash
cd eval/eval_infill
bash run_eval.sh
```

---

## ğŸ‹ï¸ äº‹å‰å­¦ç¿’

* **ãƒ‡ãƒ¼ã‚¿**: ç°¡æ½”ã§é«˜å“è³ªãªã‚³ãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‘ã‚¹[**FineCode**](https://huggingface.co/datasets/fredzzp/fine_code)ã‚’Hugging Faceã§ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°
* **åˆæœŸåŒ–**: *Dream*ã«å¾“ã„ã€**Qwen2.5-Coder**ã‹ã‚‰ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’è¡Œã„ã€æ‹¡æ•£ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«é©å¿œ
* **æå¤±**: Masked Diffusion Modelï¼ˆMDMï¼‰ç›®çš„é–¢æ•° â€” ãƒã‚¹ã‚¯æ¯”ç‡ã‚’`[0,1]`ã‹ã‚‰ä¸€æ§˜ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã§å†æ§‹æˆ

### ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
python3 scripts/download_hf_data.py --repo_id fredzzp/fine_code --local_dir ./data
```

### å­¦ç¿’

```bash
export TOKENIZERS_PARALLELISM=false
NNODES=${NNODES:=1}
NPROC_PER_NODE=4
NODE_RANK=${NODE_RANK:=0}
MASTER_ADDR=${MASTER_ADDR:=0.0.0.0}
MASTER_PORT=${MASTER_PORT:=12345}



torchrun --nnodes=$NNODES --nproc-per-node $NPROC_PER_NODE --node-rank $NODE_RANK \
  --master-addr=$MASTER_ADDR --master-port=$MASTER_PORT tasks/train_torch.py \
  configs/pretrain/qwen2_5_coder_500M.yaml \
  --data.train_path=data/data \
  --train.ckpt_manager=dcp \
  --train.micro_batch_size=16 \
  --train.global_batch_size=512 \
  --train.output_dir=logs/Qwen2.5-Coder-0.5B_mdm \
  --train.save_steps=10000
```

### Hugging Faceã¸ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

```python
from huggingface_hub import HfApi

REPO_ID = "fredzzp/open-dcoder-0.5B"
LOCAL_DIR = "logs/Qwen2.5-Coder-0.5B_mdm/checkpoints/global_step_370000/hf_ckpt"

api = HfApi()
api.create_repo(repo_id=REPO_ID, repo_type="model", exist_ok=True)
api.upload_folder(repo_id=REPO_ID, repo_type="model", folder_path=LOCAL_DIR)
```

---

## ğŸ™ è¬è¾

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ç´ æ™´ã‚‰ã—ã„å…ˆè¡Œç ”ç©¶ã®ä¸Šã«æˆã‚Šç«‹ã£ã¦ã„ã¾ã™:

* **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãƒ„ãƒ¼ãƒ«**: [VeOmni](https://github.com/ByteDance-Seed/VeOmni)ã€[lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)
* **ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹dLLM**: [LLaDA](https://github.com/ML-GSAI/LLaDA)ã€[Dream](https://github.com/HKUNLP/Dream)
* **å…ˆé§†çš„ãªdLLM**: [Gemini-Diffusion](https://deepmind.google/models/gemini-diffusion/)ã€[Seed Diffusion](https://seed.bytedance.com/seed_diffusion)ã€[Mercury](https://www.inceptionlabs.ai/introducing-mercury-our-general-chat-model)
* **åŸºç›¤ç ”ç©¶**: [MD4](https://proceedings.neurips.cc/paper_files/paper/2024/hash/bad233b9849f019aead5e5cc60cef70f-Abstract-Conference.html)ã€[MDLM](https://arxiv.org/abs/2406.07524)ã€[DPLM](https://github.com/bytedance/dplm)

ç§ãŸã¡ã¯ã“ã‚Œã‚‰ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è‚©ã®ä¸Šã«ç«‹ã£ã¦ãŠã‚Šã€Open-dLLMãŒæ‹¡æ•£LLMã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«è²¢çŒ®ã§ãã‚‹ã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚




## ğŸ“š å¼•ç”¨

**Open-dLLM**ã¾ãŸã¯**Open-dCoder**ã‚’ç ”ç©¶ã§ä½¿ç”¨ã•ã‚Œã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å¼•ç”¨ã—ã¦ãã ã•ã„:

```bibtex
@misc{opendllm2025,
  title        = {Open-dLLM: Open Diffusion Large Language Models},
  author       = {Fred Zhangzhi Peng, Shuibai Zhang, Alex Tong, and contributors},
  year         = {2025},
  howpublished = {\url{https://github.com/pengzhangzhi/Open-dLLM}},
  note         = {Blog: \url{https://oval-shell-31c.notion.site/Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a?pvs=74},
                  Model: \url{https://huggingface.co/fredzzp/open-dcoder-0.5B}}
}
```
